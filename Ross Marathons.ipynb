{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML, display\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty objects for URLs\n",
    "urlbase_2019 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2019-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_2018 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2018-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_2017 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2017-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_2016 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Marathon/2016-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2019 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2019-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2018 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2018-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2017 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2017-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n",
    "urlbase_half_2016 = 'https://www.runrocknroll.com/Events/Nashville/The-Races/Half-Marathon/2016-Results?gender=&agegroup=&bib=&firstname=&lastname=&page='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables for pagination values\n",
    "\n",
    "\n",
    "#For indexing purposes, the range inquiry needs to be 1 higher than the actual value. \n",
    "#So these variables are (actual page count + 1)\n",
    "pages_2016 = 154 + 1\n",
    "pages_2017 = 147 + 1\n",
    "pages_2018 = 85 + 1\n",
    "pages_2019 = 113 + 1\n",
    "pgs_half_2016 = 898 + 1\n",
    "pgs_half_2017 = 892 + 1\n",
    "pgs_half_2018 = 598 + 1\n",
    "pgs_half_2019 = 690 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Marathon Empty DFs\n",
    "\n",
    "marathon2016_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "marathon2017_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "marathon2018_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "marathon2019_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half Marathon Empty DFs\n",
    "\n",
    "half2016_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "half2017_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "half2018_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])\n",
    "half2019_df = pd.DataFrame(columns = ['Overall', 'Bib', 'Name', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marathon_scraper(urlbase, pag_value, df):\n",
    "    for i in range (1, pag_value):\n",
    "        url = urlbase + str(i)\n",
    "        result = requests.post(url)\n",
    "        soup = BeautifulSoup(result.text, 'lxml')\n",
    "        tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "        results_list = pd.read_html(str(tables[0]))\n",
    "        pre_df = pd.DataFrame(results_list[0])\n",
    "        df = pd.concat([df, pre_df], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2019_df = marathon_scraper(urlbase_2019, pages_2019, marathon2019_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2019_df.to_csv('data/full_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2018_df = marathon_scraper(urlbase_2018, pages_2018, marathon2018_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2018_df.to_csv('data/full_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok I'm gonna try the first for-loop to see what happens. \n",
    "# before adding the concat portion: this gets us a list, but does not append\n",
    "# after adding concat function: if axis=1, we get a weird dataframe with a ton of columns. \n",
    "# I think we're ok with axis=0 - I'll check in with help\n",
    "\n",
    "#Full 2016\n",
    "\n",
    "for i in range (1, pages_2016):\n",
    "    url = urlbase_2016 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    marathon2016_df = pd.concat([marathon2016_df, df], axis=0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2019_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2016_df.to_csv('data/full_2016.csv', index = False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm reserving this code about the table classes- I might need this later\n",
    "\n",
    "\n",
    "#<table class=\"table table-responsive table-bordered\">\n",
    "#<tr>\n",
    "#<th class=\"table-place\">Overall</th>\n",
    "#<th class=\"table-place\">Bib</th>\n",
    "#<th class=\"table-name\">Name</th>\n",
    "#<th class=\"table-time\">Time</th>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown: I'm just gonna spam this seven more times to get dataframes. Eventually I should work on a function, but let's make sure things pull ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full 2017\n",
    "\n",
    "for i in range (1, pages_2017):\n",
    "    url = urlbase_2017 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    marathon2017_df = pd.concat([marathon2017_df, df], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2017_df.to_csv('data/full_2017.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full 2018 \n",
    "for i in range (1, pages_2018):\n",
    "    url = urlbase_2018 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    marathon2018_df = pd.concat([marathon2018_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2018_df.to_csv('data/full_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Full 2019 \n",
    "for i in range (1, pages_2019):\n",
    "    url = urlbase_2019 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    marathon2019_df = pd.concat([marathon2019_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon2019_df.to_csv('data/full_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half 2016 \n",
    "for i in range (1, pgs_half_2016):\n",
    "    url = urlbase_half_2016 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    half2016_df = pd.concat([half2016_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2016_df.to_csv('data/half_2016.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Half 2017 \n",
    "for i in range (1, pgs_half_2017):\n",
    "    url = urlbase_half_2017 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    half2017_df = pd.concat([half2017_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2017_df.to_csv('data/half_2017.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half 2018 \n",
    "for i in range (1, pgs_half_2018):\n",
    "    url = urlbase_half_2018 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    half2018_df = pd.concat([half2018_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2018_df.to_csv('data/half_2018.csv', index = False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2018_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half 2019 \n",
    "for i in range (1, pgs_half_2019):\n",
    "    url = urlbase_half_2019 + str(i)\n",
    "    result = requests.post(url)\n",
    "    soup = BeautifulSoup(result.text, 'lxml')\n",
    "    tables = soup.find_all('table', attrs = {'class':\"table table-responsive table-bordered\"})\n",
    "    results_list = pd.read_html(str(tables[0]))\n",
    "    df = pd.DataFrame(results_list[0])\n",
    "    half2019_df = pd.concat([half2019_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2019_df.to_csv('data/half_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "half2019_df.head("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half2019_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
